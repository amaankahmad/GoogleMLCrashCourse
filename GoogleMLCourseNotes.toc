\contentsline {section}{\numberline {1}Framing}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Supervised Machine Learning}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Labels}{4}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Features}{4}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Examples}{4}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Models}{5}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Regression or Classification}{5}{subsection.1.6}%
\contentsline {section}{\numberline {2}Descending into ML}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}Linear Regression}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Training and Loss}{6}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Squared Loss}{6}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Mean Square Error}{6}{subsubsection.2.2.2}%
\contentsline {section}{\numberline {3}Reducing Loss}{7}{section.3}%
\contentsline {subsection}{\numberline {3.1}Iterative Approach}{7}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Gradient Decent}{8}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Stochastic Gradient Descent - Reducing Loss}{9}{subsection.3.3}%
\contentsline {section}{\numberline {4}First Steps with TensorFlow}{9}{section.4}%
\contentsline {subsection}{\numberline {4.1}Summary of hyperparameter tuning}{10}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Correlation Matrix}{11}{subsection.4.2}%
\contentsline {section}{\numberline {5}Generalisation}{11}{section.5}%
\contentsline {subsection}{\numberline {5.1}William of Ockham}{12}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Machine Learning Fine Print}{12}{subsection.5.2}%
\contentsline {section}{\numberline {6}Training and Test Sets}{13}{section.6}%
\contentsline {section}{\numberline {7}Validation}{13}{section.7}%
\contentsline {section}{\numberline {8}Representation}{14}{section.8}%
\contentsline {subsection}{\numberline {8.1}Feature Engineering}{14}{subsection.8.1}%
\contentsline {subsubsection}{\numberline {8.1.1}Mapping Numerical Values}{15}{subsubsection.8.1.1}%
\contentsline {subsubsection}{\numberline {8.1.2}Mapping Categorical Values}{15}{subsubsection.8.1.2}%
\contentsline {subsubsection}{\numberline {8.1.3}Sparse Representation}{17}{subsubsection.8.1.3}%
\contentsline {subsection}{\numberline {8.2}Qualities of Good Features}{17}{subsection.8.2}%
\contentsline {subsubsection}{\numberline {8.2.1}Avoid rarely used Discrete Feature Values}{17}{subsubsection.8.2.1}%
\contentsline {subsubsection}{\numberline {8.2.2}Provide Obvious Meanings}{18}{subsubsection.8.2.2}%
\contentsline {subsubsection}{\numberline {8.2.3}Don't mix actual data with "Magic Numbers"}{18}{subsubsection.8.2.3}%
\contentsline {subsubsection}{\numberline {8.2.4}Account for changes in definitions for Feature Values}{18}{subsubsection.8.2.4}%
\contentsline {subsection}{\numberline {8.3}Cleaning Data}{18}{subsection.8.3}%
\contentsline {subsubsection}{\numberline {8.3.1}Scaling Feature Values}{18}{subsubsection.8.3.1}%
\contentsline {subsubsection}{\numberline {8.3.2}Handling Extreme Outliers}{19}{subsubsection.8.3.2}%
\contentsline {subsubsection}{\numberline {8.3.3}Binning}{19}{subsubsection.8.3.3}%
\contentsline {subsubsection}{\numberline {8.3.4}Scrubbing}{20}{subsubsection.8.3.4}%
\contentsline {section}{\numberline {9}Feature Crosses}{20}{section.9}%
\contentsline {section}{\numberline {10}Regularisation for Simplicity}{21}{section.10}%
\contentsline {subsection}{\numberline {10.1}Lambda - The Regularization Rate}{22}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Learning rate and Lambda}{23}{subsection.10.2}%
\contentsline {section}{\numberline {11}Logistic Regression}{23}{section.11}%
\contentsline {subsection}{\numberline {11.1}Loss function for Logistic Regression}{24}{subsection.11.1}%
\contentsline {section}{\numberline {12}Classification}{24}{section.12}%
\contentsline {subsection}{\numberline {12.1}Thresholding}{24}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}True vs. False, Positve vs. Negative}{24}{subsection.12.2}%
\contentsline {subsection}{\numberline {12.3}Accuracy, Precision and Recall}{24}{subsection.12.3}%
\contentsline {subsection}{\numberline {12.4}ROC and AUC}{25}{subsection.12.4}%
\contentsline {subsection}{\numberline {12.5}Prediction Bias}{25}{subsection.12.5}%
\contentsline {section}{\numberline {13}Regularization: Sparsity}{26}{section.13}%
\contentsline {section}{\numberline {14}Neural Networks}{26}{section.14}%
\contentsline {subsection}{\numberline {14.1}Hidden Layers}{27}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Activation Functions}{28}{subsection.14.2}%
\contentsline {subsection}{\numberline {14.3}Common Activation Functions}{28}{subsection.14.3}%
\contentsline {section}{\numberline {15}Best Practices when training neural networks}{29}{section.15}%
\contentsline {subsection}{\numberline {15.1}Failure Cases}{29}{subsection.15.1}%
\contentsline {subsubsection}{\numberline {15.1.1}Vanishing Gradients}{29}{subsubsection.15.1.1}%
\contentsline {subsubsection}{\numberline {15.1.2}Exploding Gradients}{29}{subsubsection.15.1.2}%
\contentsline {subsubsection}{\numberline {15.1.3}Dead ReLU Units}{29}{subsubsection.15.1.3}%
\contentsline {subsection}{\numberline {15.2}Dropout Regularization}{29}{subsection.15.2}%
