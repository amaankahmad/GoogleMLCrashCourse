\BOOKMARK [1][-]{section.1}{Framing}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Supervised Machine Learning}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Labels}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Features}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Examples}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Models}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{Regression or Classification}{section.1}% 7
\BOOKMARK [1][-]{section.2}{Descending into ML}{}% 8
\BOOKMARK [2][-]{subsection.2.1}{Linear Regression}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.2}{Training and Loss}{section.2}% 10
\BOOKMARK [3][-]{subsubsection.2.2.1}{Squared Loss}{subsection.2.2}% 11
\BOOKMARK [3][-]{subsubsection.2.2.2}{Mean Square Error}{subsection.2.2}% 12
\BOOKMARK [1][-]{section.3}{Reducing Loss}{}% 13
\BOOKMARK [2][-]{subsection.3.1}{Iterative Approach}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.2}{Gradient Decent}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.3}{Stochastic Gradient Descent - Reducing Loss}{section.3}% 16
\BOOKMARK [1][-]{section.4}{First Steps with TensorFlow}{}% 17
\BOOKMARK [2][-]{subsection.4.1}{Summary of hyperparameter tuning}{section.4}% 18
\BOOKMARK [2][-]{subsection.4.2}{Correlation Matrix}{section.4}% 19
\BOOKMARK [1][-]{section.5}{Generalisation}{}% 20
\BOOKMARK [2][-]{subsection.5.1}{William of Ockham}{section.5}% 21
\BOOKMARK [2][-]{subsection.5.2}{Machine Learning Fine Print}{section.5}% 22
\BOOKMARK [1][-]{section.6}{Training and Test Sets}{}% 23
\BOOKMARK [1][-]{section.7}{Validation}{}% 24
\BOOKMARK [1][-]{section.8}{Representation}{}% 25
\BOOKMARK [2][-]{subsection.8.1}{Feature Engineering}{section.8}% 26
\BOOKMARK [3][-]{subsubsection.8.1.1}{Mapping Numerical Values}{subsection.8.1}% 27
\BOOKMARK [3][-]{subsubsection.8.1.2}{Mapping Categorical Values}{subsection.8.1}% 28
\BOOKMARK [3][-]{subsubsection.8.1.3}{Sparse Representation}{subsection.8.1}% 29
\BOOKMARK [2][-]{subsection.8.2}{Qualities of Good Features}{section.8}% 30
\BOOKMARK [3][-]{subsubsection.8.2.1}{Avoid rarely used Discrete Feature Values}{subsection.8.2}% 31
\BOOKMARK [3][-]{subsubsection.8.2.2}{Provide Obvious Meanings}{subsection.8.2}% 32
\BOOKMARK [3][-]{subsubsection.8.2.3}{Don't mix actual data with "Magic Numbers"}{subsection.8.2}% 33
\BOOKMARK [3][-]{subsubsection.8.2.4}{Account for changes in definitions for Feature Values}{subsection.8.2}% 34
\BOOKMARK [2][-]{subsection.8.3}{Cleaning Data}{section.8}% 35
\BOOKMARK [3][-]{subsubsection.8.3.1}{Scaling Feature Values}{subsection.8.3}% 36
\BOOKMARK [3][-]{subsubsection.8.3.2}{Handling Extreme Outliers}{subsection.8.3}% 37
\BOOKMARK [3][-]{subsubsection.8.3.3}{Binning}{subsection.8.3}% 38
\BOOKMARK [3][-]{subsubsection.8.3.4}{Scrubbing}{subsection.8.3}% 39
\BOOKMARK [1][-]{section.9}{Feature Crosses}{}% 40
\BOOKMARK [1][-]{section.10}{Regularisation for Simplicity}{}% 41
\BOOKMARK [2][-]{subsection.10.1}{Lambda - The Regularization Rate}{section.10}% 42
\BOOKMARK [2][-]{subsection.10.2}{Learning rate and Lambda}{section.10}% 43
\BOOKMARK [1][-]{section.11}{Logistic Regression}{}% 44
\BOOKMARK [2][-]{subsection.11.1}{Loss function for Logistic Regression}{section.11}% 45
\BOOKMARK [1][-]{section.12}{Classification}{}% 46
\BOOKMARK [2][-]{subsection.12.1}{Thresholding}{section.12}% 47
\BOOKMARK [2][-]{subsection.12.2}{True vs. False, Positve vs. Negative}{section.12}% 48
\BOOKMARK [2][-]{subsection.12.3}{Accuracy, Precision and Recall}{section.12}% 49
\BOOKMARK [2][-]{subsection.12.4}{ROC and AUC}{section.12}% 50
\BOOKMARK [2][-]{subsection.12.5}{Prediction Bias}{section.12}% 51
\BOOKMARK [1][-]{section.13}{Regularization: Sparsity}{}% 52
\BOOKMARK [1][-]{section.14}{Neural Networks}{}% 53
\BOOKMARK [2][-]{subsection.14.1}{Hidden Layers}{section.14}% 54
\BOOKMARK [2][-]{subsection.14.2}{Activation Functions}{section.14}% 55
\BOOKMARK [2][-]{subsection.14.3}{Common Activation Functions}{section.14}% 56
\BOOKMARK [1][-]{section.15}{Best Practices when training neural networks}{}% 57
\BOOKMARK [2][-]{subsection.15.1}{Failure Cases}{section.15}% 58
\BOOKMARK [3][-]{subsubsection.15.1.1}{Vanishing Gradients}{subsection.15.1}% 59
\BOOKMARK [3][-]{subsubsection.15.1.2}{Exploding Gradients}{subsection.15.1}% 60
\BOOKMARK [3][-]{subsubsection.15.1.3}{Dead ReLU Units}{subsection.15.1}% 61
\BOOKMARK [2][-]{subsection.15.2}{Dropout Regularization}{section.15}% 62
